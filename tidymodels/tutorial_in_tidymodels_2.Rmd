---
title: "Tutorial on Tidymodel part II"
output: md_document
---

Reproduzindo este [artigo](https://hansjoerg.me/2020/02/09/tidymodels-for-machine-learning/) sobre `tidymodels` do [Hansjörg Plieninger](https://hansjoerg.me/page/about/)

# Tutorial on tidymodels for Machine Learning

## Setup

```{r loadlibs, message=FALSE, warning=FALSE}
library(conflicted)
library(tidymodels)
library(tidyverse)
library(ggrepel)
library(corrplot)
library(skimr)

conflict_prefer("filter","dplyr")

ggplot2::theme_set(theme_light())
```

## Dataset: `Diamonds`

```{r diamond_plot}

data("diamonds")

head(diamonds,10)

# you know: the life, the universer... 
set.seed(42)

# training/test
dia_split <- initial_split(diamonds, prop = .1, strata = price)
dia_train <- training(dia_split)
dia_test  <- testing(dia_split)

dim(dia_train)
dim(dia_test)

# cv-folds
dia_vfold <- vfold_cv(dia_train, v=3, repeats=1, strata=price)


dia_vfold %>% 
  mutate(df_ana=map(splits, analysis),
         df_ass=map(splits, assessment)) -> folds

```

## Fitando uma RF

### Modelo: `parsnip`

we use `tune()` as a placeholder and let cross-validation decide on the best value for `mtry` later on.

```{r rfmodel}

# define the model
rf_model <- rand_forest(mtry = tune()) %>% #mtry will be tuned
  set_mode("regression") %>% # we'll fit for a value
  set_engine("ranger") # the RF algo

# check parameters
parameters(rf_model)

dials::mtry()

```

You can either specify the maximum for `mtry` yourself using `update()`, or you can use `finalize()` to let the data decide on the maximum.

```{r}

# setting the mtry range by hand
rf_model %>% 
  parameters() %>% 
  update(mtry=mtry(c(1L, 5L)))


# rf_model %>% 
#     parameters() %>% 
#     # Here, the maximum of mtry equals the number of predictors, i.e., 24.
#     finalize(x = select(juice(prep(dia_rec)), -price)) %>% str()
#     pull("object")

```
## Preparing data for tunning: `recipes`

As we create a feature using `step_poly()` we need to decide what are the best degree, let's `tune()` this too.

```{r}

dia_rec2 <- recipe(price~., data = dia_train) %>% 
  step_log(all_outcomes()) %>% 
  step_normalize(all_predictors(), -all_nominal()) %>% 
  step_dummy(all_nominal()) %>% 
  step_poly(carat, degree = tune())

dia_rec2 %>% 
  parameters() %>% 
  pull("object")

```

## Combine everything: `workflows`

The workflows package is designed to bundle together different parts of a machine learning pipeline like a recipe or a model.

```{r}

rf_wflow <-
    workflow() %>%
    add_model(rf_model) %>%
    add_recipe(dia_rec2)

rf_wflow

```

Second, we need to update the parameters in `rf_wflow`, because the maximum of `mtry` is not yet known and the maximum of `degree` should be four (while three is the default).

```{r}

rf_param <-
    rf_wflow %>%
    parameters() %>%
    update(mtry = mtry(range = c(3L, 5L)),
           degree = degree_int(range = c(2L, 4L)))

```


Third, we want to use cross-validation for tuning, that is, to select the best combination of the hyperparameters. Grid search will suffice. To this end, let’s create a grid of all necessary parameter combinations.

```{r}

rf_grid <- grid_regular(rf_param)
rf_grid

```

Make sure parallel processing and do a grid search

```{r}

library(future)
plan(multicore)


rf_search <- tune_grid(rf_wflow, grid = rf_grid, resamples = dia_vfold,
                       param_info = rf_param)

rf_search

```


The results can be examined using `autoplot()` and `show_best()`.

```{r}

autoplot(rf_search, metric="rmse")
show_best(rf_search, metric = "rmse",n=9) #top nine
select_best(rf_search, metric = "rmse")
select_by_one_std_err(rf_search, mtry, degree, metric="rmse")

```

## Selecting the Best Model to Make the Final Predictions

```{r}
# uses the "one-standard error rule" (Breiman _el at, 1984) 
# that selects the most simple model that is within one standard error
# of the numerically optimal results.
rf_param_final <- select_by_one_std_err(rf_search, mtry, degree, metric = "rmse")

# set 
rf_wflow_final <- finalize_workflow(rf_wflow, rf_param_final)

rf_wflow_final_fit <- fit(rf_wflow_final, data=dia_train)
rf_wflow_final_fit
```

## Predict

Now, we want to use this to `predict()` on data never seen before, namely, `dia_test`. Unfortunately, `predict(rf_wflow_final_fit, new_data = dia_test)` does not work in the present case, because the outcome is modified in the recipe via `step_log()`.

Thus, we need a little workaround: The prepped recipe is extracted from the workflow, and this can then be used to `bake()` the testing data. This baked data set together with the extracted model can then be used for the final predictions.

```{r}

# recipe from final fit
dia_rec3 <- pull_workflow_prepped_recipe(rf_wflow_final_fit)

# engine from final fit
rf_final_fit <- pull_workflow_fit(rf_wflow_final_fit)

# predicts using final fit engine with dia_test baked with final recipe
# add the y_hat to the test dataset
dia_test$.pred <- predict(rf_final_fit, new_data = bake(dia_rec3, dia_test))$.pred

# transform the Y into a log 
# apply metrics
dia_test %>%
  select(price, .pred) %>% 
  mutate(price.log = log(price)) %>% 
  metrics(truth=price.log, estimate=.pred)
```

